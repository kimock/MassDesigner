import torch
import torch.nn as nn
import torch_geometric as tg
from util import *
from util_graph import get_program_ratio


import torch
import torch.nn as nn

class VolumetricDesignLoss_D(nn.Module):
    def __init__(self, gan_loss, gp_lambda=10):
        super(VolumetricDesignLoss_D, self).__init__()
        self.gan_loss = gan_loss
        self.gp_lambda = gp_lambda

    def forward(self, real_validity_voxel, fake_validity_voxel, gp=None):
        if self.gan_loss == "WGANGP":
            Dv_loss = -torch.mean(real_validity_voxel[0]) - torch.mean(real_validity_voxel[1]) + torch.mean(fake_validity_voxel[0]) + torch.mean(fake_validity_voxel[1])
            return Dv_loss + (self.gp_lambda * gp if gp is not None else 0)

        device = real_validity_voxel[0].get_device()
        valid0 = torch.FloatTensor(real_validity_voxel[0].shape[0], 1).fill_(1.0).to(device)
        valid1 = torch.FloatTensor(real_validity_voxel[1].shape[0], 1).fill_(1.0).to(device)
        fake0 = torch.FloatTensor(fake_validity_voxel[0].shape[0], 1).fill_(0.0).to(device)
        fake1 = torch.FloatTensor(fake_validity_voxel[1].shape[0], 1).fill_(0.0).to(device)

        if self.gan_loss == "NSGAN":  # NS GAN  log(D(x))+log(1-D(G(z)))
            loss = nn.BCELoss()
            return loss(real_validity_voxel[0], valid0) + loss(real_validity_voxel[1], valid1) + loss(fake_validity_voxel[0], fake0) + loss(fake_validity_voxel[1], fake1)
        elif self.gan_loss == "LSGAN":  # LS GAN  (D(x)-1)^2 + (D(G(z)))^2
            loss = nn.MSELoss()
            return 0.5 * (loss(real_validity_voxel[0], valid0) + loss(real_validity_voxel[1], valid1) + loss(fake_validity_voxel[0], fake0) + loss(fake_validity_voxel[1], fake1))
        elif self.gan_loss == "hinge":  # SA GAN
            loss = nn.ReLU()
            return loss(1.0 - real_validity_voxel[0]).mean() + loss(1.0 - real_validity_voxel[1]).mean() + loss(fake_validity_voxel[0] + 1.0).mean() + loss(fake_validity_voxel[1] + 1.0).mean()
        else:
            raise TypeError("self.gan_loss is not valid")


import torch
import torch.nn as nn
from util import *
from util_graph import get_program_ratio

class VolumetricDesignLoss_G(nn.Module):
    def __init__(self, lp_weight, tr_weight, far_weight, embedding_dim, sample_size, similarity_fun, gan_loss, lp_loss, hinge_margin):
        super(VolumetricDesignLoss_G, self).__init__()
        self.gan_loss = gan_loss
        self.tr_weight = tr_weight
        self.lp_weight = lp_weight
        self.far_weight = far_weight

    def forward(self, fake_validity_voxel, graph, att, mask, area_index_in_voxel_feature):
        device = att.get_device() if att.is_cuda else "cpu"

        # ê¸°ì¡´ Loss êµ¬ì„±
        target0 = torch.FloatTensor(fake_validity_voxel[0].shape[0], 1).fill_(1.0).to(device)
        target1 = torch.FloatTensor(fake_validity_voxel[1].shape[0], 1).fill_(1.0).to(device)

        if self.gan_loss == "WGANGP":
            adversarial_loss = -torch.mean(fake_validity_voxel[0]) - torch.mean(fake_validity_voxel[1])
        elif self.gan_loss == "NSGAN":
            loss = nn.BCELoss()
            adversarial_loss = loss(fake_validity_voxel[0], target0) + loss(fake_validity_voxel[1], target1)
        elif self.gan_loss == "LSGAN":
            loss = nn.MSELoss()
            adversarial_loss = loss(fake_validity_voxel[0], target0) + loss(fake_validity_voxel[1], target1)
        elif self.gan_loss == "hinge":
            adversarial_loss = -torch.mean(fake_validity_voxel[0]) - torch.mean(fake_validity_voxel[1])

        # Program Ratio Loss & FAR Loss
        normalized_program_class_weight, _, FAR = get_program_ratio(graph, att, mask, area_index_in_voxel_feature)
        target_ratio_loss = self.tr_weight * nn.functional.smooth_l1_loss(normalized_program_class_weight.flatten(), graph.program_target_ratio)
        far_diff = FAR.view(FAR.shape[0]) - graph.FAR  # FAR ì°¨ì´ ê³„ì‚°
        smooth_loss = nn.functional.smooth_l1_loss(FAR.view(FAR.shape[0]), graph.FAR)  # ê¸°ì¡´ FAR ì†ì‹¤


        # **(1) FAR ì´ˆê³¼ ê°ì§€**
        # far_exceed = torch.clamp(far_diff, min=0)  # ì´ˆê³¼ëŸ‰ë§Œ ì¶”ì¶œ (ìŒìˆ˜ëŠ” 0ìœ¼ë¡œ)
        
        # **(2) ì´ˆê³¼ëŸ‰ì— ëŒ€í•´ ê°•í•œ íŒ¨ë„í‹° ì ìš©**
        # far_penalty = (far_exceed * 2).mean()  # 3ì œê³±ìœ¼ë¡œ ì´ˆê³¼ ì‹œ ê¸‰ê²©ížˆ ì¦ê°€

        # **(3) ì ì ˆí•œ ê°€ì¤‘ì¹˜ ì¡°ì ˆ**
        # total_far_loss = self.far_weight * (smooth_loss + far_penalty)
        total_far_loss = self.far_weight * smooth_loss

        # ðŸ”¹ Valid Connection Loss ë°˜ì˜ (ë¯¸ë¶„ ê°€ëŠ¥!)
        total_loss = adversarial_loss + target_ratio_loss + total_far_loss

        return total_loss, adversarial_loss, target_ratio_loss, total_far_loss



def compute_gradient_penalty(Dv, batch, label, out):
    # Interpolated sample
    device = out.get_device()
    u = torch.FloatTensor(label.shape[0], 1).uniform_(0, 1).to(device)  # weight between model and gt label
    mixed_sample = torch.autograd.Variable(label * u + out * (1 - u), requires_grad=True).to(device)  # Nv x C
    mask = (mixed_sample.max(dim=-1)[0] != 0).type(torch.float32).view(-1, 1)
    sample = softmax_to_hard(mixed_sample, -1) * mask

    # compute gradient penalty
    dv_loss = Dv(batch, sample)
    grad_b = torch.autograd.grad(outputs=dv_loss[0], inputs=sample, grad_outputs=torch.ones(dv_loss[0].shape).to(device), retain_graph=True, create_graph=True, only_inputs=True)[0]
    grad_s = torch.autograd.grad(outputs=dv_loss[1], inputs=sample, grad_outputs=torch.ones(dv_loss[1].shape).to(device), retain_graph=True, create_graph=True, only_inputs=True)[0]
    dv_gp_b = ((grad_b.norm(2, 1) - 1) ** 2).mean()
    dv_gp_s = ((grad_s.norm(2, 1) - 1) ** 2).mean()
    dv_gp = dv_gp_b + dv_gp_s

    return dv_gp, dv_gp_b, dv_gp_s

def compute_vertical_alignment_loss(graph, out_hard, voxel_pos_index=(3,4), stair_label=2, elevator_label=3):
    """
    graph.voxel_feature: (N_voxel, D) e.g. D>=6 (dimension[0..2], coordinate[3..5], etc.)
    out_hard: (N_voxel, num_programs) 1-hot
    voxel_floor_cluster: (N_voxel,) floor index
    stair_label: 2, elevator_label: 3 (ì‚¬ìš©ì¤‘ì¸ í”„ë¡œê·¸ëž¨ ë¼ë²¨)
    
    - ëª©í‘œ: 
      ì¸µ iì™€ i+1ì— ëŒ€í•´, stairë¼ë¦¬, elevatorë¼ë¦¬ x-y ì¢Œí‘œ ì°¨ê°€ ìž‘ë„ë¡
    """
    device = out_hard.device
    # (A) ì¶”ì¶œ: ë³µì…€ë³„ x, y ì¢Œí‘œ
    # ì˜ˆ: voxel_featureì—ì„œ x=3, y=4 ë¼ê³  ê°€ì •
    x = graph.voxel_feature[:, voxel_pos_index[0]]
    y = graph.voxel_feature[:, voxel_pos_index[1]]

    # (B) ì–´ë–¤ ë³µì…€ì´ stair/elevator ì¸ì§€ ë§ˆìŠ¤í¬
    # out_hard[i, stair_label]==1ì´ë©´ stair
    # out_hard[i, elevator_label]==1ì´ë©´ elevator
    stair_mask = (out_hard[:, stair_label] == 1)
    elev_mask = (out_hard[:, elevator_label] == 1)

    # (C) floor ì •ë³´
    floors = graph.voxel_floor_cluster  # (N_voxel,) = ê° ë³µì…€ì´ ëª‡ ë²ˆ ì¸µì¸ì§€

    # (D) ìˆ˜ì§ ì •ë ¬ ë¡œìŠ¤ ê³„ì‚°
    # ì•„ì´ë””ì–´: ì¸µ iì™€ i+1ì˜ stair ë³µì…€ë“¤ ê°„ì— (x_i - x_{i+1})^2 + (y_i - y_{i+1})^2 ìµœì†Œí™”
    # ê°„ë‹¨ížˆ BFS/ë§¤ì¹­ ë“±ì„ í•  ìˆ˜ë„ ìžˆì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ "floor iì˜ centroid" vs "floor i+1ì˜ centroid" ì‹ìœ¼ë¡œ ì ‘ê·¼
    vertical_loss_stair = torch.tensor(0.0, device=device)
    vertical_loss_elev = torch.tensor(0.0, device=device)
    count_stair, count_elev = 0, 0

    max_floor = int(floors.max().item())

    for f in range(max_floor):
        # fì¸µ stair ë³µì…€ë“¤ì˜ (x,y) í‰ê· 
        f_mask = (floors == f)
        next_mask = (floors == f+1)
        
        # (1) ìŠ¤í…Œì–´
        stair_f = f_mask & stair_mask  # í˜„ìž¬ ì¸µ stair
        stair_f_next = next_mask & stair_mask  # ë‹¤ìŒ ì¸µ stair
        if stair_f.sum() > 0 and stair_f_next.sum() > 0:
            x_mean_cur = x[stair_f].mean()
            y_mean_cur = y[stair_f].mean()
            x_mean_next = x[stair_f_next].mean()
            y_mean_next = y[stair_f_next].mean()
            dist = (x_mean_cur - x_mean_next)**2 + (y_mean_cur - y_mean_next)**2
            vertical_loss_stair += dist
            count_stair += 1

        # (2) ì—˜ë¦¬ë² ì´í„°
        elev_f = f_mask & elev_mask
        elev_f_next = next_mask & elev_mask
        if elev_f.sum() > 0 and elev_f_next.sum() > 0:
            x_mean_cur = x[elev_f].mean()
            y_mean_cur = y[elev_f].mean()
            x_mean_next = x[elev_f_next].mean()
            y_mean_next = y[elev_f_next].mean()
            dist = (x_mean_cur - x_mean_next)**2 + (y_mean_cur - y_mean_next)**2
            vertical_loss_elev += dist
            count_elev += 1

    # í‰ê· í™”
    if count_stair > 0:
        vertical_loss_stair /= count_stair
    if count_elev > 0:
        vertical_loss_elev /= count_elev

    vertical_loss = vertical_loss_stair + vertical_loss_elev
    return vertical_loss